{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f07eb148",
   "metadata": {},
   "source": [
    "## Movie Review Dataset\n",
    "we will be using an IMDB movie review dataset from keras with 25,000 reviews where each one is already preprocessed as either a positive or negative. #Each review is encoded by integers that represent how common a word is in the entire dataset. E.g. a word encoded by int 3 means it is the 3rd most common word in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16ed2d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 132s 8us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence #outmoded - keras.utils import sequence\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "VOCAB_SIZE = 88584\n",
    "\n",
    "MAXLEN = 250\n",
    "BATCH_SIZE =64\n",
    "(train_data, train_lbls), (test_data,test_lbls) = imdb.load_data(num_words = VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec2a11bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check out the first review of our train datasets\n",
    "len(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d6d5a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# len of all reviews are not the same\n",
    "print(len(train_data[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6dd4bf",
   "metadata": {},
   "source": [
    "### More Preprocessing\n",
    "with our reviews havn diff lengths, we cant pass such data into our neural network. therefore, we make each review the same length with the basic criteria:\n",
    "\n",
    "i. if the review is greater than 250 words, cut off the excess.\n",
    "ii. if review is less than 250 words, add the needed amount of zeros to make it sum up to 250\n",
    "\n",
    "Keras has a function for such tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d1fcc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = sequence.pad_sequences(train_data, MAXLEN)\n",
    "# test_data = sequence.pad_sequences(test_data, MAXLEN)              #changed path of the pad_sequiences\n",
    "\n",
    "train_data = tf.keras.utils.pad_sequences(train_data, MAXLEN)\n",
    "test_data = tf.keras.utils.pad_sequences(test_data, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9eccd026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n",
      "250\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data[0]))\n",
    "print(len(train_data[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e17332",
   "metadata": {},
   "source": [
    "### Creating the Model\n",
    "Now we create our model, with a word embedding layer as teh first layer in our model and then add the LSTM layer afterwards  to feed into a dense node to get our predicted sentiment.\n",
    "\n",
    "32 stands for the output dim of the vectors generated by the embedding layer. we can change teh value if we want.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cff287f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 32)          2834688   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 32)                8320      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,843,041\n",
      "Trainable params: 2,843,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, 32), #creates 32 dims of the resulting vector\n",
    "    tf.keras.layers.LSTM(32), #telss lstm later it ll need 2 add 32 dims to each word... \n",
    "    tf.keras.layers.Dense(1,activation = 'sigmoid') #activation fxn squezes vals btn 0 and 1\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e6e73c",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99a66e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 157s 231ms/step - loss: 0.4419 - acc: 0.7914 - val_loss: 0.3667 - val_acc: 0.8462\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 117s 188ms/step - loss: 0.2521 - acc: 0.9038 - val_loss: 0.2724 - val_acc: 0.8886\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 117s 187ms/step - loss: 0.1922 - acc: 0.9287 - val_loss: 0.2690 - val_acc: 0.8962\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 117s 186ms/step - loss: 0.1585 - acc: 0.9429 - val_loss: 0.2732 - val_acc: 0.8874\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 115s 184ms/step - loss: 0.1361 - acc: 0.9510 - val_loss: 0.3235 - val_acc: 0.8738\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 137s 219ms/step - loss: 0.1202 - acc: 0.9594 - val_loss: 0.3062 - val_acc: 0.8932\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 125s 200ms/step - loss: 0.1044 - acc: 0.9654 - val_loss: 0.3103 - val_acc: 0.8918\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 125s 201ms/step - loss: 0.0920 - acc: 0.9686 - val_loss: 0.3113 - val_acc: 0.8786\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 139s 222ms/step - loss: 0.0829 - acc: 0.9729 - val_loss: 0.3181 - val_acc: 0.8912\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 120s 192ms/step - loss: 0.0728 - acc: 0.9763 - val_loss: 0.3450 - val_acc: 0.8858\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer = 'rmsprop', metrics = ['acc'])\n",
    "history = model.fit(train_data, train_lbls, epochs = 10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afb5bb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 49s 62ms/step - loss: 0.4155 - acc: 0.8590\n",
      "[0.4154852330684662, 0.859000027179718]\n"
     ]
    }
   ],
   "source": [
    "#Now we evaluate the model on our training data to see how well it performs\n",
    "results = model.evaluate(test_data, test_lbls)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a0602b",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "Now we will use our network to make predictions on our reviews.\n",
    "as they are encoded we will need to convert any review that we write into that form so the network can understand it. for that pupose we will load the encodings from the dataset and use them to encode our data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a4f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode function\n",
    "word_index = imdb.get_word_index() #these are the get wor  indices of our movie review ds\n",
    "def encode_text(text):\n",
    "    tokens = keras.preprocesing.text.text_to_word_sequence(text) #converts our strings to series of words csv\n",
    "    tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
    "    #if word in our tokens exist in our imdb ds, then we put that coresponding val into our tokens as a replacement, else 0\n",
    "    return keras.utils.pad_sequences([tokens], MAXLEN)[0] #the return statement returns our shit with teh pad sequences which\n",
    "#ultimately sets the string to a cap of 250, the pad sequences returns a list and our tokens val is a list so a list of list \n",
    "#is returned, that is why we added the [0], so that one list is returned.\n",
    "\n",
    "varstring = 'that movie was simply amazing, so amazing'\n",
    "encodede = encode_text(varstring)\n",
    "print(encodede)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75156a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode fxn\n",
    "reverse_word_index = {value: key for (key, val) in word_index.items()}\n",
    "\n",
    "def decode_integers(integers):\n",
    "    PAD = 0\n",
    "    text = \"\"\n",
    "    for num in integers:\n",
    "        if num != PAD:\n",
    "            text += reverse_word_index[num] +\" \"\n",
    "    return text[:-1] #returns all but the last space in our string\n",
    "\n",
    "\n",
    "print(decode_integers(encodede))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056effdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time to see a prediction\n",
    "def predict(text):\n",
    "    encoded_text = encode_text(text)\n",
    "    pred = np.zeros((1, 250))\n",
    "    pred[0] = encoded_text\n",
    "    result = model.predict(pred)\n",
    "    print(result[0])\n",
    "    \n",
    "positive = 'That was so amazing! I really loved it and would watch it again because it was amazingly great'\n",
    "predict(positive)\n",
    "\n",
    "neg = 'that movie sucked. I hated it and would never watch it again. Was one of the worst things i have ever watched'\n",
    "predict(neg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
